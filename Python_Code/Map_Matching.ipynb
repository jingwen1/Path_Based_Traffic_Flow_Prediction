{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stratoskar/Path-Based-Traffic-Flow-Prediction/blob/main/Python_Code/Map_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d28908",
      "metadata": {
        "id": "01d28908"
      },
      "outputs": [],
      "source": [
        "# Handle data files\n",
        "import pandas as pd\n",
        "\n",
        "# These libraries needed for communicating with the Map Matching API\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "from datetime import timedelta\n",
        "\n",
        "# These libraries are for connecting to the database\n",
        "import psycopg2\n",
        "from sqlalchemy import create_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128a9283",
      "metadata": {
        "id": "128a9283"
      },
      "outputs": [],
      "source": [
        "# Read all data\n",
        "all_data = pd.read_csv('splitted_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f76b2b1",
      "metadata": {
        "id": "4f76b2b1"
      },
      "outputs": [],
      "source": [
        "# Sort all data\n",
        "all_data = all_data.sort_values(['Taxi ID','Traj ID','Date Time'])\n",
        "all_data = all_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4daa06",
      "metadata": {
        "id": "7f4daa06"
      },
      "outputs": [],
      "source": [
        "# View data\n",
        "all_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "526a1992",
      "metadata": {
        "id": "526a1992"
      },
      "outputs": [],
      "source": [
        "# Count unique pairs of columns 'Traj ID' AND 'Taxi ID'\n",
        "unique_pairs = all_data[['Taxi ID', 'Traj ID']].drop_duplicates()\n",
        "count_unique_pairs = len(unique_pairs)\n",
        "print(f\"Number of trajectories in the dataset: {count_unique_pairs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7602ede",
      "metadata": {
        "id": "b7602ede"
      },
      "outputs": [],
      "source": [
        "# Convert string values to datetime\n",
        "all_data['Date Time'] = pd.to_datetime(all_data['Date Time'])\n",
        "\n",
        "print(\"Min date is: \"+str(all_data['Date Time'].min()))\n",
        "print(\"Max date is: \"+str(all_data['Date Time'].max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d9da08",
      "metadata": {
        "id": "b0d9da08"
      },
      "outputs": [],
      "source": [
        "# Pass latitude and longitude pairs to Valhalla API\n",
        "df_for_meili = all_data[['Latitude','Longitude']]\n",
        "df_for_meili = df_for_meili.rename(columns={\"Latitude\": \"lat\", \"Longitude\": \"lon\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcb1a236",
      "metadata": {
        "id": "dcb1a236"
      },
      "source": [
        "### Map matching done using Valhalla Meili API.\n",
        "\n",
        "Given each trajectory to the API as input, the response contains information of the exact path that each trajectory followed. The paths are in the form of OSM Way IDs.\n",
        "\n",
        "#### Sources:\n",
        "\n",
        "Installation using Docker: https://ikespand.github.io/posts/meili/\n",
        "Paper about Valhalla: https://link.springer.com/article/10.1007/s42979-022-01340-5#Tab5\n",
        "APIs documentation: https://valhalla.github.io/valhalla/api/map-matching/api-reference/#matched-point-items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b653d1",
      "metadata": {
        "id": "32b653d1"
      },
      "outputs": [],
      "source": [
        "# For each Taxi ID in the dataset\n",
        "for taxi_id in all_data['Taxi ID'].unique():\n",
        "    # For each Traj ID in the dataset\n",
        "    for traj_id in all_data[all_data['Taxi ID'] == taxi_id]['Traj ID'].unique():\n",
        "\n",
        "        # Define a new dataframe to store the map matched results\n",
        "        visited_segments = pd.DataFrame(columns=['taxi_id', 'traj_id', 'osm_way_id', 'edge_length', 'edge_speed',\n",
        "       'road_class', 'expected_start_time', 'expected_end_time'])\n",
        "\n",
        "        # Get the batch of data that we send to the request\n",
        "        indexes = all_data[(all_data['Taxi ID']==taxi_id) & (all_data['Traj ID'] == traj_id)].index\n",
        "\n",
        "        # Input to API\n",
        "        passed_data = df_for_meili.iloc[indexes]\n",
        "\n",
        "        # Preparing the request to Valhalla's Meili\n",
        "        meili_coordinates = passed_data.to_json(orient='records')\n",
        "\n",
        "        # Head and Tail of the request\n",
        "        meili_head = '{\"shape\":'\n",
        "        meili_tail = \"\"\",\"search_radius\": 200, \"sigma_z\": 10, \"beta\": 10,\"shape_match\":\"map_snap\", \"costing\":\"auto\",\n",
        "                        \"filters\":{\"attributes\":[\"edge.way_id\",\"edge.speed\",\"edge.length\",\"edge.road_class\"],\"action\":\"include\"},\n",
        "                        \"format\":\"osrm\"}\"\"\"\n",
        "\n",
        "        # Construction of the the request\n",
        "        meili_request_body = meili_head + meili_coordinates + meili_tail\n",
        "\n",
        "        # The URL of the local valhalla server\n",
        "        url = \"http://localhost:8002/trace_attributes\"\n",
        "\n",
        "        # Providing headers to the request\n",
        "        headers = {'Content-type': 'application/json'}\n",
        "\n",
        "        # we need to send the JSON as a string\n",
        "        data = str(meili_request_body)\n",
        "\n",
        "        # sending a request\n",
        "        r = requests.post(url, data=data, headers=headers)\n",
        "\n",
        "        if r.status_code == 200: # Response from Valhalla API was successful\n",
        "            # Connect to database\n",
        "            connection = psycopg2.connect(database=\"visited_segments\",\n",
        "                                              user=\"postgres\",\n",
        "                                              password=\"sobadata2\",\n",
        "                                              host=\"localhost\",\n",
        "                                              port=\"5432\")\n",
        "\n",
        "            # Create a cursor to execute queries\n",
        "            cursor = connection.cursor()\n",
        "\n",
        "            # This is the name of the table to be created inside the database\n",
        "            name = \"table_\"+str(taxi_id)+'_'+str(traj_id)\n",
        "\n",
        "            # Create the table if it doesn't exist\n",
        "            cursor.execute(f\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS {name} (\n",
        "                Taxi_ID VARCHAR(255) NOT NULL,\n",
        "                Traj_ID VARCHAR(255) NOT NULL,\n",
        "                OSM_Way_ID VARCHAR(255) NOT NULL,\n",
        "                Edge_Length VARCHAR(255) NOT NULL,\n",
        "                Edge_Speed VARCHAR(255) NOT NULL,\n",
        "                Road_Class VARCHAR(255) NOT NULL,\n",
        "                Expected_Start_Time VARCHAR(255) NOT NULL,\n",
        "                Expected_End_Time VARCHAR(255) NOT NULL\n",
        "            )\n",
        "            \"\"\")\n",
        "\n",
        "            # Commit the changes to the database\n",
        "            connection.commit()\n",
        "\n",
        "            # Parsing the JSON response\n",
        "            response_text = json.loads(r.text)\n",
        "\n",
        "            # Find the time interval (in sec) that the trajectory needs to be completed [last timestamp - first timestamp]\n",
        "            interval = (all_data.iloc[indexes].iloc[-1]['Date Time'] - all_data.iloc[indexes].iloc[0]['Date Time']).total_seconds()\n",
        "\n",
        "            # Compute the expected duration that the moving object is in each edge (duration is equal for each edge that the trajectory visits)\n",
        "            duration  = interval/len(response_text['edges'])\n",
        "\n",
        "            # Fill the rows of the dataframe with information that API gave\n",
        "            for i in range(len(response_text['edges'])):\n",
        "                visited_segments.at[i,'taxi_id'] = taxi_id # taxi id\n",
        "                visited_segments.at[i,'traj_id'] = traj_id # traj id\n",
        "                visited_segments.at[i,'osm_way_id'] = response_text['edges'][i]['way_id'] # osm_way id\n",
        "                visited_segments.at[i,'edge_speed'] = response_text['edges'][i]['speed'] # speed\n",
        "                visited_segments.at[i,'edge_length'] = response_text['edges'][i]['length'] # edges length\n",
        "                visited_segments.at[i,'road_class'] = response_text['edges'][i]['road_class'] # type of edge\n",
        "                # time information\n",
        "                if i == 0:\n",
        "                    visited_segments.at[i,'expected_start_time'] = all_data.iloc[indexes].iloc[0]['Date Time']\n",
        "                else:\n",
        "                    visited_segments.at[i,'expected_start_time'] = visited_segments.at[i-1,'expected_end_time']\n",
        "\n",
        "                visited_segments.at[i,'expected_end_time'] = visited_segments.at[i,'expected_start_time'] + timedelta(seconds=duration)\n",
        "\n",
        "            # Convert all dataframe to string\n",
        "            visited_segments = visited_segments.astype(str)\n",
        "\n",
        "            # Try to pass this information to database table\n",
        "            try:\n",
        "                engine = create_engine('postgresql://postgres:sobadata2@localhost:5432/visited_segments')\n",
        "\n",
        "                visited_segments.to_sql(name, engine, if_exists='append', index=False)\n",
        "\n",
        "            except (Exception, psycopg2.Error) as error:\n",
        "                print(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the map matching process are stored in a PostgreSQL database, due to the huge volume of data that is produced. In the next code files, we download the data from the database, aggregate them and construct a single CSV file with all the data gathered together."
      ],
      "metadata": {
        "id": "QXqZOpzlLgp1"
      },
      "id": "QXqZOpzlLgp1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python DataAnalytics",
      "language": "python",
      "name": "dataanalytics"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}