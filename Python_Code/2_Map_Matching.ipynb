{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stratoskar/Path_Based_Traffic_Flow_Prediction/blob/main/Python_Code/2_Map_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code file we perform the Map Matching operation. In other words, we are trying to map every GPS record to the actual position inside the road network.\n"
      ],
      "metadata": {
        "id": "qLw3T89F1p0n"
      },
      "id": "qLw3T89F1p0n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d28908",
      "metadata": {
        "id": "01d28908"
      },
      "outputs": [],
      "source": [
        "# Handle data files\n",
        "import pandas as pd\n",
        "\n",
        "# These libraries are essential for the communication with the Map Matching API\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "\n",
        "# Handle timestamp values\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o6oEj5cKxDnf"
      },
      "id": "o6oEj5cKxDnf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read split trajectory data\n",
        "all_data = pd.read_csv('splitted_data.csv')"
      ],
      "metadata": {
        "id": "R8Q8QCZSw-Xo"
      },
      "id": "R8Q8QCZSw-Xo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f76b2b1",
      "metadata": {
        "id": "4f76b2b1"
      },
      "outputs": [],
      "source": [
        "# Sort all data\n",
        "all_data = all_data.sort_values(['taxi_id','traj_id','Date Time'])\n",
        "all_data = all_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert string values to datetime\n",
        "all_data['Date Time'] = pd.to_datetime(all_data['Date Time'])"
      ],
      "metadata": {
        "id": "05BWo6niw7k2"
      },
      "id": "05BWo6niw7k2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Begin Map Matching Operations using Valhalla Meili API**"
      ],
      "metadata": {
        "id": "PoYyrLa_JCp0"
      },
      "id": "PoYyrLa_JCp0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d9da08",
      "metadata": {
        "id": "b0d9da08"
      },
      "outputs": [],
      "source": [
        "# Pass latitude and longitude pairs to Valhalla API\n",
        "df_for_meili = all_data[['Latitude','Longitude']]\n",
        "df_for_meili = df_for_meili.rename(columns={\"Latitude\": \"lat\", \"Longitude\": \"lon\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b653d1",
      "metadata": {
        "id": "32b653d1"
      },
      "outputs": [],
      "source": [
        "# Define a new dataframe to store the Map Matched data\n",
        "visited_segments = pd.DataFrame(columns=['taxi_id', 'traj_id', 'osm_way_id', 'edge_length', 'edge_speed', 'road_class', 'expected_start_time', 'expected_end_time'])\n",
        "\n",
        "# For each 'taxi_id' in the dataset\n",
        "for taxi_id in all_data['taxi_id'].unique():\n",
        "\n",
        "    # For each 'traj_id' in the dataset\n",
        "    for traj_id in all_data[all_data['taxi_id'] == taxi_id]['traj_id'].unique():\n",
        "\n",
        "        # Get the batch of data that we send to the request\n",
        "        indexes = all_data[(all_data['taxi_id']==taxi_id) & (all_data['traj_id'] == traj_id)].index\n",
        "\n",
        "        # Input to API\n",
        "        passed_data = df_for_meili.iloc[indexes]\n",
        "\n",
        "        # Preparing the request to Valhalla's Meili\n",
        "        meili_coordinates = passed_data.to_json(orient='records')\n",
        "\n",
        "        # Head and Tail of the request\n",
        "        meili_head = '{\"shape\":'\n",
        "        meili_tail = \"\"\",\"search_radius\": 200, \"sigma_z\": 10, \"beta\": 10,\"shape_match\":\"map_snap\", \"costing\":\"auto\",\n",
        "                        \"filters\":{\"attributes\":[\"edge.way_id\",\"edge.speed\",\"edge.length\",\"edge.road_class\"],\"action\":\"include\"},\n",
        "                        \"format\":\"osrm\"}\"\"\"\n",
        "\n",
        "        # Construction of the the request\n",
        "        meili_request_body = meili_head + meili_coordinates + meili_tail\n",
        "\n",
        "        # The URL of the local valhalla server\n",
        "        url = \"http://localhost:8002/trace_attributes\"\n",
        "\n",
        "        # Providing headers to the request\n",
        "        headers = {'Content-type': 'application/json'}\n",
        "\n",
        "        # we need to send the JSON as a string\n",
        "        data = str(meili_request_body)\n",
        "\n",
        "        # Sending a request to Valhalla Meili API\n",
        "        r = requests.post(url, data=data, headers=headers)\n",
        "\n",
        "        # Response from Valhalla API was successful\n",
        "        if r.status_code == 200:\n",
        "\n",
        "            # Parse the JSON response\n",
        "            response_text = json.loads(r.text)\n",
        "\n",
        "            # Find the time interval (in sec) that the trajectory needs to be completed [last timestamp - first timestamp]\n",
        "            interval = (all_data.iloc[indexes].iloc[-1]['Date Time'] - all_data.iloc[indexes].iloc[0]['Date Time']).total_seconds()\n",
        "\n",
        "            # Compute the expected duration that the moving object is in each edge (duration is equal for each edge that the trajectory visits)\n",
        "            duration  = interval/len(response_text['edges'])\n",
        "\n",
        "             # Define a temporary dataframe to store temporary information\n",
        "            temp = pd.DataFrame(columns=['taxi_id', 'traj_id', 'osm_way_id', 'edge_length', 'edge_speed','road_class', 'expected_start_time', 'expected_end_time'])\n",
        "\n",
        "            # Fill the rows of the dataframe with information that API gave\n",
        "            for i in range(len(response_text['edges'])):\n",
        "                temp.at[i,'taxi_id'] = taxi_id # taxi id\n",
        "                temp.at[i,'traj_id'] = traj_id # traj id\n",
        "                temp.at[i,'osm_way_id'] = response_text['edges'][i]['way_id'] # osm_way id\n",
        "                temp.at[i,'edge_speed'] = response_text['edges'][i]['speed'] # speed\n",
        "                temp.at[i,'edge_length'] = response_text['edges'][i]['length'] # edges length\n",
        "                temp.at[i,'road_class'] = response_text['edges'][i]['road_class'] # type of edge\n",
        "\n",
        "                # Time information\n",
        "                if i == 0:\n",
        "                    temp.at[i,'expected_start_time'] = all_data.iloc[indexes].iloc[0]['Date Time']\n",
        "                else:\n",
        "                    temp.at[i,'expected_start_time'] = temp.at[i-1,'expected_end_time']\n",
        "\n",
        "                temp.at[i,'expected_end_time'] = temp.at[i,'expected_start_time'] + timedelta(seconds=duration)\n",
        "\n",
        "            # Concatenate the two dataframes\n",
        "            visited_segments = pd.concat([visited_segments,temp],ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "During Map Matching, we considered that the time interval that a Yellow Taxi is within each edge is equal for every edge in the trajectory. This concept is wrong. The following code tries to estimate 'expected_start_time' and 'expected_end_time' for each edge with respect to the length of this edge."
      ],
      "metadata": {
        "id": "65p6zOfT98FJ"
      },
      "id": "65p6zOfT98FJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# For each 'taxi_id' in the dataset\n",
        "for taxi_id in visited_segments['taxi_id'].unique():\n",
        "\n",
        "    # For each 'traj_id' within the current 'taxi_id'\n",
        "    for traj_id in visited_segments[visited_segments['taxi_id'] == taxi_id]['traj_id'].unique():\n",
        "\n",
        "        # Get the data of a specific trajectory\n",
        "        current_trajectory = visited_segments[\n",
        "            (visited_segments['taxi_id'] == taxi_id) & (visited_segments['traj_id'] == traj_id)\n",
        "        ]\n",
        "\n",
        "        # Get the indexes of that specific trajectory\n",
        "        indexes = visited_segments[\n",
        "            (visited_segments['taxi_id'] == taxi_id) & (visited_segments['traj_id'] == traj_id)\n",
        "        ].index.to_list()\n",
        "\n",
        "        # Calculate the sum of lengths of all edges present in this trajectory\n",
        "        sum_of_lengths = sum(current_trajectory['edge_length'])\n",
        "\n",
        "        # Create a new column with the percentages of the lengths of each edge to the sum of all edges\n",
        "        current_trajectory['length_percentage'] = (current_trajectory['edge_length'] / sum_of_lengths)\n",
        "\n",
        "        # Based on the percentages that we calculated,\n",
        "        # define the time interval that the vehicle was in a specific edge of the current trajectory\n",
        "        time_enter_index = current_trajectory.index[0]\n",
        "        time_leave_index = current_trajectory.index[-1]\n",
        "\n",
        "        # Compute the whole time interval of the selected trajectory\n",
        "        complete_interval = (\n",
        "            current_trajectory.at[time_leave_index, 'expected_end_time']\n",
        "            - current_trajectory.at[time_enter_index, 'expected_start_time']\n",
        "        ).seconds\n",
        "\n",
        "        # Update the start and end times using with respect to the length of each edge\n",
        "        i = 0\n",
        "        try:\n",
        "            for index in indexes:\n",
        "                if i == 0:\n",
        "                    # Update 'expected_end_time' for the first index\n",
        "                    visited_segments.at[index, 'expected_end_time'] = visited_segments.at[index, 'expected_start_time'] + timedelta(seconds=(current_trajectory.at[index, 'length_percentage'] * complete_interval))\n",
        "                else:\n",
        "                    # Update 'expected_start_time' and 'expected_end_time' for subsequent indexes\n",
        "                    visited_segments.at[index, 'expected_start_time'] = visited_segments.at[index - 1, 'expected_end_time']\n",
        "                    visited_segments.at[index, 'expected_end_time'] = visited_segments.at[index, 'expected_start_time'] + timedelta(seconds=(current_trajectory.at[index, 'length_percentage'] * complete_interval))\n",
        "                i = i + 1\n",
        "        except Exception as e:\n",
        "            print(f'Problem at trajectory:{taxi_id},{traj_id}')\n",
        "            print(e)\n",
        "            pass"
      ],
      "metadata": {
        "id": "2Js0INJe9XcQ"
      },
      "id": "2Js0INJe9XcQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data to csv\n",
        "visited_segments.to_csv('interpolated_data.csv', index=False)"
      ],
      "metadata": {
        "id": "eB22N1z8_B1B"
      },
      "id": "eB22N1z8_B1B",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python DataAnalytics",
      "language": "python",
      "name": "dataanalytics"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}